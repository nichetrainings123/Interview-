I am a data engineer with over [X years] of experience in designing, developing, and optimizing large-scale data platforms, with a strong specialization in Databricks, Apache Spark, and Delta Lake for building end-to-end, production-grade data pipelines within medallion architectures. My core expertise lies in leveraging Databricks to architect scalable and reliable ETL/ELT workflows using SQL and Python, implementing Delta Lake for efficient storage and retrieval with ACID compliance, and building high-performance batch and streaming pipelines using Spark Structured Streaming and Kafka/Event Hubs. I have extensively worked across multi-cloud ecosystems—AWS, Azure, and GCP—deploying pipelines with native cloud services like AWS Glue, Azure Data Factory, and GCP Dataflow, while integrating data storage solutions such as S3, ADLS, and GCS. I focus heavily on optimization for performance, reliability, and cost-efficiency, ensuring pipelines are tuned for parallelism, partitioning, and caching strategies within Databricks. I have implemented robust data governance and lineage frameworks using Unity Catalog, incorporating RBAC and data security best practices, and used MLflow extensively for experiment tracking, model registry, and deployment of ML models directly within the Databricks environment. A significant part of my work has been bridging data engineering with machine learning, collaborating with AI/ML teams to operationalize models, implement feature stores, and build agentic workflows leveraging Databricks Agents integrated with LLMs like Anthropic Claude, LLaMA, and Google Gemini, as well as frameworks such as Hugging Face Transformers, LangChain, and LlamaIndex. I have developed pipelines that support both structured and unstructured data processing, powering AI-driven insights and BI solutions for downstream consumption. My technical experience also extends to DevOps practices where I have set up CI/CD workflows for Databricks notebooks, SQL scripts, and ML pipelines using Databricks Repos integrated with GitHub, GitLab, and Azure Repos, alongside GitHub Actions and Azure DevOps for automation. I am comfortable with infrastructure-as-code and have worked on enabling production-ready deployment frameworks with automated testing, linting, and deployment approvals. I am certified as a Databricks Data Engineer Associate/Professional and have hands-on experience implementing the lakehouse paradigm with Delta Lake, ensuring scalable ingestion with Auto Loader, schema evolution, and time-travel features for reproducibility. I have also worked on real-time analytics pipelines, integrating Kafka streams with Spark Structured Streaming for low-latency data processing, powering use cases like fraud detection, predictive analytics, and monitoring systems. Beyond core data engineering, I bring practical exposure to MLOps practices where I have designed workflows for continuous training, retraining, and monitoring of ML models in production, leveraging MLflow for version control of models and integrating with cloud-native ML services. I have implemented monitoring frameworks to track pipeline SLAs, data drift, and model performance, troubleshooting production issues proactively to ensure high availability and reliability. My focus has always been on balancing scalability with cost efficiency, for example, by applying cluster auto-scaling, leveraging Delta caching, and right-sizing compute clusters. I also bring experience in feature engineering and deploying reusable feature pipelines, enabling AI teams to rapidly experiment while ensuring consistency of features across training and inference. More recently, I have been experimenting with integrating Databricks Agents into BI and AI workflows, building automated agents that leverage third-party LLMs to perform context-aware data transformations, generate insights, and support decision automation. My background combines strong hands-on engineering with an understanding of the ML lifecycle, making me effective in cross-functional teams where collaboration between data, ML, and business stakeholders is critical. I thrive in environments where innovation is encouraged, and I take pride in being able to translate business needs into scalable technical solutions, from data modeling and pipeline design to production-ready AI/ML integrations. Overall, I bring a mix of strong Databricks, Spark, and Delta Lake expertise, cloud-native data engineering, governance with Unity Catalog, operationalization of AI/ML with MLflow and Databricks Agents, and DevOps best practices, positioning me to deliver value in mission-critical data lakehouse and AI-driven analytics environments like yours.
